# R represent the number of samples
# burn_in is the number of discarded samples
# S is the covariance matrix of the multivariate Gaussian proposal
RMH_Adaptive <- function(R, burn_in, y, X) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
epsilon <- 1e-6 # Inital value for the covariance matrix
# Initial matrix S
S <- diag(epsilon, p)
Sigma_r <- diag(0, p)
mu_r <- beta
for (r in 1:(burn_in + R)) {
# Updating the covariance matrix
if(r > 1){
Sigma_r <- (r - 2) / (r - 1) * Sigma_r + tcrossprod(beta - mu_r) / r
mu_r <- (r - 1) / r * mu_r + beta / r
S <- 2.38^2 * Sigma_r / p + diag(epsilon, p)
}
# Eigen-decomposition
eig <- eigen(S, symmetric = TRUE)
A1 <- t(eig$vectors) * sqrt(eig$values)
beta_new <- beta + c(matrix(rnorm(p), 1, p) %*% A1)
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta <- beta_new # Accept the value
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH_Adaptive(R = R, burn_in = burn_in, y, X))
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[3, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
plot(fit_MCMC[, 1])
burn_in <- 0
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH_Adaptive(R = R, burn_in = burn_in, y, X))
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[3, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
plot(fit_MCMC[, 1])
RMH_Gibbs <- function(R, burn_in, y, X, se) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- beta_new <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
for (r in 1:(burn_in + R)) {
for (j in 1:p) {
beta_new[j] <- beta[j] + rnorm(1, 0, se[j])
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta[j] <- beta_new[j] # Accept the value
}
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
RMH_Gibbs_Adaptive <- function(R, burn_in, y, X, target = 0.44) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- beta_new <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
epsilon <- rep(0, p) # Initial value
accepted <- numeric(p) # Vector of accepted values
batch <- 1
for (r in 1:(burn_in + R)) {
# Do we need to update the parameters?
if (batch == 50) {
for (j in 1:p) {
# Adapting the standard errors
if ((accepted[j] / 50) > target) {
epsilon[j] <- epsilon[j] + min(0.01, sqrt(1 / r))
} else {
epsilon[j] <- epsilon[j] - min(0.01, sqrt(1 / r))
}
}
# Restart the cycle - Erase everything
accepted <- numeric(p) # Vector of accepted values
batch <- 0
}
# Increment the batch
batch <- batch + 1
for (j in 1:p) {
beta_new[j] <- beta[j] + rnorm(1, 0, exp(epsilon[j]))
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta[j] <- beta_new[j] # Accept the value
accepted[j] <- accepted[j] + 1
}
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH_Gibbs_Adaptive(R = R, burn_in = burn_in, y, X)) # Convert the matrix into a "coda" object
end.time <- Sys.time()
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[5, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
# Chunk 1
Pima <- rbind(MASS::Pima.tr, MASS::Pima.te)
# Chunk 2
y <- as.numeric(Pima$type == "Yes") # Binary outcome
X <- model.matrix(type ~ . - 1, data = Pima) # Design matrix
X <- cbind(1, X)
# X <- cbind(1, scale(X)) # Standardize the design matrix, add the intercept
# Chunk 3
# Probit model
fit_probit <- glm(type ~ X - 1, family = binomial(link = "probit"), data = Pima)
# Logit model
fit_logit <- glm(type ~ X - 1, family = binomial(link = "logit"), data = Pima)
# Chunk 4
# Loglikelihood of a logistic regression model
loglik <- function(beta, y, X) {
eta <- c(X %*% beta)
sum(y * eta - log(1 + exp(eta)))
}
# Logposterior
logpost <- function(beta, y, X) {
loglik(beta, y, X) + sum(dnorm(beta, 0, 10, log = T))
}
# Chunk 5
# R represent the number of samples
# burn_in is the number of discarded samples
# S is the covariance matrix of the multivariate Gaussian proposal
RMH <- function(R, burn_in, y, X, S) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
# Eigen-decomposition
eig <- eigen(S, symmetric = TRUE)
A1 <- t(eig$vectors) * sqrt(eig$values)
# Starting the Gibbs sampling
for (r in 1:(burn_in + R)) {
beta_new <- beta + c(matrix(rnorm(p), 1, p) %*% A1)
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta <- beta_new # Accept the value
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
# Chunk 6
library(coda)
R <- 30000 # Number of retained samples
burn_in <- 5000 # Burn-in period
# Summary table
summary_tab <- matrix(0, nrow = 5, ncol = 4)
colnames(summary_tab) <- c("Seconds", "Average ESS", "Average ESS per sec", "Average acceptance rate")
rownames(summary_tab) <- c("Vanilla MH", "Laplace MH", "Adaptive MH", "Metropolis within Gibbs", "Adaptive Metropolis within Gibbs")
# Chunk 7
set.seed(123)
# Covariance matrix of the proposal
S <- diag(1e-3, ncol(X))
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH(R, burn_in, y, X, S)) # Convert the matrix into a "coda" object
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[1, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of a couple of parameters
plot(fit_MCMC[, 3:4])
# Chunk 1
Pima <- rbind(MASS::Pima.tr, MASS::Pima.te)
# Chunk 2
y <- as.numeric(Pima$type == "Yes") # Binary outcome
X <- model.matrix(type ~ . - 1, data = Pima) # Design matrix
X <- cbind(1, X)
# X <- cbind(1, scale(X)) # Standardize the design matrix, add the intercept
# Chunk 3
# Probit model
fit_probit <- glm(type ~ X - 1, family = binomial(link = "probit"), data = Pima)
# Logit model
fit_logit <- glm(type ~ X - 1, family = binomial(link = "logit"), data = Pima)
# Chunk 4
# Loglikelihood of a logistic regression model
loglik <- function(beta, y, X) {
eta <- c(X %*% beta)
sum(y * eta - log(1 + exp(eta)))
}
# Logposterior
logpost <- function(beta, y, X) {
loglik(beta, y, X) + sum(dnorm(beta, 0, 10, log = T))
}
# Chunk 5
# R represent the number of samples
# burn_in is the number of discarded samples
# S is the covariance matrix of the multivariate Gaussian proposal
RMH <- function(R, burn_in, y, X, S) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
# Eigen-decomposition
eig <- eigen(S, symmetric = TRUE)
A1 <- t(eig$vectors) * sqrt(eig$values)
# Starting the Gibbs sampling
for (r in 1:(burn_in + R)) {
beta_new <- beta + c(matrix(rnorm(p), 1, p) %*% A1)
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta <- beta_new # Accept the value
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
# Chunk 6
library(coda)
R <- 30000 # Number of retained samples
burn_in <- 5000 # Burn-in period
# Summary table
summary_tab <- matrix(0, nrow = 5, ncol = 4)
colnames(summary_tab) <- c("Seconds", "Average ESS", "Average ESS per sec", "Average acceptance rate")
rownames(summary_tab) <- c("Vanilla MH", "Laplace MH", "Adaptive MH", "Metropolis within Gibbs", "Adaptive Metropolis within Gibbs")
# Chunk 7
set.seed(123)
# Covariance matrix of the proposal
S <- diag(1e-3, ncol(X))
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH(R, burn_in, y, X, S)) # Convert the matrix into a "coda" object
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[1, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of a couple of parameters
plot(fit_MCMC[, 3:4])
# Chunk 8
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
# Covariance matrix is selected via laplace approximation
fit_logit <- glm(type ~ X - 1, family = binomial(link = "logit"), data = Pima)
p <- ncol(X)
S <- 2.38^2 * vcov(fit_logit) / p
# MCMC
fit_MCMC <- as.mcmc(RMH(R, burn_in, y, X, S)) # Convert the matrix into a "coda" object
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[2, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
# Chunk 9
# R represent the number of samples
# burn_in is the number of discarded samples
# S is the covariance matrix of the multivariate Gaussian proposal
RMH_Adaptive <- function(R, burn_in, y, X) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
epsilon <- 1e-6 # Inital value for the covariance matrix
# Initial matrix S
S <- diag(epsilon, p)
Sigma_r <- diag(0, p)
mu_r <- beta
for (r in 1:(burn_in + R)) {
# Updating the covariance matrix
if(r > 1){
Sigma_r <- (r - 2) / (r - 1) * Sigma_r + tcrossprod(beta - mu_r) / r
mu_r <- (r - 1) / r * mu_r + beta / r
S <- 2.38^2 * Sigma_r / p + diag(epsilon, p)
}
# Eigen-decomposition
eig <- eigen(S, symmetric = TRUE)
A1 <- t(eig$vectors) * sqrt(eig$values)
beta_new <- beta + c(matrix(rnorm(p), 1, p) %*% A1)
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta <- beta_new # Accept the value
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
# Chunk 10
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH_Adaptive(R = R, burn_in = burn_in, y, X))
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[3, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
# Chunk 11
RMH_Gibbs <- function(R, burn_in, y, X, se) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- beta_new <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
for (r in 1:(burn_in + R)) {
for (j in 1:p) {
beta_new[j] <- beta[j] + rnorm(1, 0, se[j])
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta[j] <- beta_new[j] # Accept the value
}
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
# Chunk 12
p <- ncol(X)
se <- sqrt(rep(1e-4, p))
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH_Gibbs(R = R, burn_in = burn_in, y, X, se)) # Convert the matrix into a "coda" object
end.time <- Sys.time()
time_in_sec <- as.numeric(end.time - start.time)
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[4, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
# Chunk 13
RMH_Gibbs_Adaptive <- function(R, burn_in, y, X, target = 0.44) {
p <- ncol(X)
out <- matrix(0, R, p) # Initialize an empty matrix to store the values
beta <- beta_new <- rep(0, p) # Initial values
logp <- logpost(beta, y, X)
epsilon <- rep(0, p) # Initial value
accepted <- numeric(p) # Vector of accepted values
batch <- 1
for (r in 1:(burn_in + R)) {
# Do we need to update the parameters?
if (batch == 50) {
for (j in 1:p) {
# Adapting the standard errors
if ((accepted[j] / 50) > target) {
epsilon[j] <- epsilon[j] + min(0.01, sqrt(1 / r))
} else {
epsilon[j] <- epsilon[j] - min(0.01, sqrt(1 / r))
}
}
# Restart the cycle - Erase everything
accepted <- numeric(p) # Vector of accepted values
batch <- 0
}
# Increment the batch
batch <- batch + 1
for (j in 1:p) {
beta_new[j] <- beta[j] + rnorm(1, 0, exp(epsilon[j]))
logp_new <- logpost(beta_new, y, X)
alpha <- min(1, exp(logp_new - logp))
if (runif(1) < alpha) {
logp <- logp_new
beta[j] <- beta_new[j] # Accept the value
accepted[j] <- accepted[j] + 1
}
}
# Store the values after the burn-in period
if (r > burn_in) {
out[r - burn_in, ] <- beta
}
}
out
}
# Chunk 14
set.seed(123)
# Running the MCMC
start.time <- Sys.time()
fit_MCMC <- as.mcmc(RMH_Gibbs_Adaptive(R = R, burn_in = burn_in, y, X)) # Convert the matrix into a "coda" object
end.time <- Sys.time()
# Diagnostic
summary(effectiveSize(fit_MCMC)) # Effective sample size
summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time
summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate
# Summary statistics
summary_tab[5, ] <- c(
time_in_sec, mean(effectiveSize(fit_MCMC)),
mean(effectiveSize(fit_MCMC)) / time_in_sec,
1 - mean(rejectionRate(fit_MCMC))
)
# Traceplot of the intercept
plot(fit_MCMC[, 1:2])
library(Rcpp)
library(RcppArmadillo)
# In this case the file is saved in a different folder.
# Obviously, you need to change the PATH as needed
sourceCpp("../cpp/sum.cpp")
setwd("~/Library/Mobile Documents/com~apple~CloudDocs/University/Didattica/Computational Statistics/CompStat/exe")
# In this case the file is saved in a different folder.
# Obviously, you need to change the PATH as needed
sourceCpp("../cpp/sum.cpp")
install.packages("Rcpp")
Rcpp::sourceCpp("RMH.cpp")
