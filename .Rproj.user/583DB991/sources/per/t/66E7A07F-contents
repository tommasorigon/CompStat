---
title: "Computational Statistics II"
subtitle: "Homework 1"
author:
  name: Tommaso Rigon
  affiliation: DEMS
page-layout: full
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
    execute:
      echo: true
      message: false
      warning: false
editor_options: 
  chunk_output_type: console
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = T, eval = T, message = F, warning = F, error = F, comment = NA, cache = F, include = T, R.options = list(width = 100), collapse = T, dpi = 200, fig.align = "center", fig.height = 5, fig.width = 8)
```

The homeworks are not graded, but the results may be sent to tommaso.rigon@unimib.it to receive feedbacks. 

## Introduction

In this homework we consider the **CAGE cancer dataset** that is described in the tutorial [**genes_tutorial.md**](https://github.com/danieledurante/ProbitSUN/blob/master/genes_tutorial.md). This is the same dataset  that is presented in the article [Durante (2019), *Conjugate Bayes for Probit Regression via Unified Skew-Normal Distributions*](https://academic.oup.com/biomet/article/106/4/765/5554418). 


**Note**. This is a high-dimensional problem, so adjust your expectations about the performance of the algorithms accordingly. 

### Dataset description (from the `gene_tutorial.md` document)

The focus is on learning how gene expression (monitored at $p - 1 = 516$ tags) relates to the probability of a cancerous tissue. Data are available for $n = 74$ measurements and can be downloaded at Cancer SAGE by clicking [here](http://www.i3s.unice.fr/~pasquier/web/userfiles/downloads/datasets/SAGE_filtered_small_dataset.zip). The download provides a directory `SAGE_filtered_small_dataset` which contains several datasets. Here the focus is on `dataset_74-516.csv`. 


```{r}
rm(list = ls())
dataset_gene <- read.csv("dataset_74-516.csv", header = TRUE, sep = "")
```

The dataframe  `dataset_gene` contains information on the **response variable** (first column of `dataset_gene`), and on the **covariates** (subsequent columns of `dataset_gene`). More specifically, the first column `dataset_gene[, 1]` contains names of tissues followed by a letter which is either *N* (normal) or *C* (cancerous). Exploiting this information, **let us create the response by hand**.

```{r}
y_data <- c(
  0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
  1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,
  0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1
)
```

The design matrix comprising the covariates can be easily obtained by extracting the remaining columns in `dataset_gene`. Following [Gelman et al. (2008)](https://projecteuclid.org/euclid.aoas/1231424214), **such covariates are also rescaled and an intercept term is added**.

```{r}
X_data <- cbind(1, scale(dataset_gene[, -1]))
```

## Homework

### Model description

Let $\textbf{y} = (y_1,\dots,y_n)^\intercal$ be the vector of the observed **binary responses** (variable `y_data`) and let $\textbf{X}$ be the corresponding **design matrix** (object `X_data`) whose generic row is $\textbf{x}_i = (x_{i1},\dots,x_{ip})^\intercal$, for $i=1,\dots,n$, suitably standardized. Consider a generalized linear model such that

$$
y_i \mid \pi_i \overset{\text{ind}}{\sim} \text{Bern}(\pi_i), \qquad \pi_i = g(\eta_i), \qquad \eta_i = \beta_1x_{i1} + \cdots + \beta_p x_{ip},
$$
where $g(\cdot)$ is the **probit link** (`pnorm` function) and let the **prior** be $\beta \sim N(0, 16 I_p)$.  

### Assignments

1. Implement a **random walk Metropolis** algorithm for sampling from the posterior distribution of $\beta$. Tune the covariance matrix of the Gaussian proposal distribution by trial and error. 

1. Obtain a rough estimate of the posterior covariance matrix using the Laplace approximation; refer to [Chopin & Ridgway (2017)](https://projecteuclid.org/journals/statistical-science/volume-32/issue-1/Leave-Pima-Indians-Alone--Binary-Regression-as-a-Benchmark/10.1214/16-STS581.full) for the details. Then, run a **random walk Metropolis** based on this approximation. 

1. Implement a **MALA** algorithm for the sampling from the posterior distribution of $\beta$ based approximation for the covariance matrix obtained in the previous point.
