---
title: "Bayesian computations"
subtitle: "Homework 2"
author:
  name: Tommaso Rigon
  affiliation: DEMS
page-layout: full
format:
  html:
    html-math-method: katex
    echo: true
    callout-appearance: minimal
    theme: [simplex, ../template.css]
    toc: true
    embed-resources: false
    code-line-numbers: true
    smooth-scroll: true
    fig-dpi: 250
    execute:
      echo: true
      message: false
      warning: false
editor_options: 
  chunk_output_type: console
---

The homeworks are not graded, but the results may be sent to tommaso.rigon@unimib.it to receive feedbacks. 

## Pima indian dataset

In this homework we consider once again the **Pima indian dataset**, as in the previous [**Markdown document B.1**](un_B1.html). Importantly, note that in this homework we will **not standardize the predictors** to make the computational problem more challenging. 

```{r}
Pima <- rbind(MASS::Pima.tr, MASS::Pima.te)
y <- as.numeric(Pima$type == "Yes") # Binary outcome
X <- cbind(1, model.matrix(type ~ . - 1, data = Pima)) # Design matrix
```



## Homework

### Model description

Let $\textbf{y} = (y_1,\dots,y_n)^\intercal$ be the vector of the observed **binary responses** (variable `y_data`) and let $\textbf{X}$ be the corresponding **design matrix** (object `X_data`) whose generic row is $\textbf{x}_i = (x_{i1},\dots,x_{ip})^\intercal$, for $i=1,\dots,n$, suitably standardized. Consider a generalized linear model such that

$$
y_i \mid \pi_i \overset{\text{ind}}{\sim} \text{Bern}(\pi_i), \qquad \pi_i = \Phi(\eta_i), \qquad \eta_i = \beta_1x_{i1} + \cdots + \beta_p x_{ip},
$$
where $\Phi(\cdot)$ is the **probit link** (`pnorm` function).As done in [**Markdown document B.1**](un_B1.html), we will employ a relatively vague prior centered at $0$, namely $\beta \sim N(0, 100 I_p)$. 

### Assignments

1. Implement the **Albert and Chib (1993)** data augmentation algorithm for sampling from the posterior distribution of $\beta$. 
