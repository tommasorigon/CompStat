---
title: "Computational Statistics II"
subtitle: "Ph.D. in Economics, Statistics, and Data Science - University of Milano-Bicocca"
author:
  name: Tommaso Rigon
  affiliation: DEMS
format:
  html:
    theme: cosmo
    toc: true
    embed-resources: true
    code-line-numbers: true
    smooth-scroll: true
    execute:
      echo: true
      message: false
      warning: false
editor_options: 
  chunk_output_type: console
---


Computational Statistics II is a course on **Bayesian computations**. The course covers both theoretical and programming aspects. 

## Prerequisites

The course is **modular**, meaning that depending on the background of the participants different units will be covered. However, it is assumed the knowledge of the following topics:

  * Fundamentals of Bayesian statistics. Refer to Chapters 1-5 of Hoff (2009).
  * Monte Carlo integration. Refer to Chapter 3 of Robert and Casella (2009), or Chapter 4 of Hoff (2009).

#### Preliminary references

 * Hoff, P. D. (2009). _A First Course in Bayesian Statistical Methods_. Springer.
 * Robert, C. P., and Casella, G. (2009). [_Introducing Monte Carlo methods with R_](https://link.springer.com/content/pdf/10.1007%2F978-1-4419-1576-4.pdf). Springer.
 
 Further preliminary material is available (in Italian) at the website of the course [R for the multivariate statistical analysis](https://tommasorigon.github.io/introR/). 

## Teaching material

| Topic | Slides | Code |
|---------------------|--------------------|------|
| | **MCMC methods** | |
| Metropolis-Hastings and Gibbs sampling | [Unit A.1](slides/un_A1.pdf) | [Code A.1](exe/un_A1.html) |
| Rcpp & RcppArmadillo | [Unit A.2](slides/un_A2.pdf) | [Code A.2](exe/un_A2.html) |
| | **Advanced MCMC algorithms** | |
| Optimal scaling & adaptive Metropolis |  [Unit B.1](slides/un_B1.pdf) |  [Code B.1](exe/un_B1.html) | 
| MALA algorithm & Hamiltonian Monte Carlo |  [Unit B.2](slides/un_B2.pdf) | [Code B.2](exe/un_B2.html) | 
| *Homework* |  | [Homework 1](exe/hom_1.html)|  |
| | **Data augmentation** | |
| Missing data problems, Gibbs sampling and the EM algorithm |  [Unit C.1](slides/un_C1.pdf)| 
| Data augmentation for probit and logit models |  [Unit C.2](slides/un_C2.pdf) | [Code B.2](exe/un_B2.html) 
| | | [EM logistic tutorial](https://github.com/tommasorigon/logisticVB/blob/master/em_logistic_tutorial.md) | 
| *Homework* |  | [Homework 2](exe/hom_2.html)| |
| | **Approximate Bayesian methods** | |
| Laplace appr., Variational Bayes, and Expectation Propagation |  [Unit D.1](slides/un_D1.pdf) | 
| Approximate methods for probit and logit models |  [Unit D.2](slides/un_D2.pdf) | [Code D.2](exe/un_D2.html) |

## Essential references

1. Albert, J. H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous response data. *Journal of the American Statistical Association*, **88**(422), 669--679. 

1. Blei, D. M., Kucukelbirb A., and McAuliffe, J. D. (2017). Variational inference: a review for statisticians. *Journal of the American Statistical Association*, **112**(518), 859--877.

1. Chopin, N. and Ridgway, J. (2017). Leave Pima indians alone: binary regression as a benchmark for Bayesian computation. *Statistical Science*, **32**(1), 64--87.

1. Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. *Journal of the Royal Statistical Society. Series B: Statistical Methodology*, **39**(1), 1--38.

1. Durante, D. (2019). Conjugate Bayes for probit regression via unified skew-normal distributions. *Biometrika*, **106**(4), 765--779.

1. Durante, D. and Rigon, T. (2019). Conditionally conjugate mean-field variational Bayes for logistic models. *Statistical Science*, **34**(3), 472--485.

1. Dunson, D. B. and Johndrow, J. E. (2020). The Hastings algorithm at fifthy. *Biometrika*, **107**(1), 1--23.

1. Eddelbuettel, D. and Balamuta, J. J. (2018). Extending R with C++: a brief introduction to Rcpp. *The American Statistician*, **72**(1), 28--36.

1. Hunter, D. R., and Lange, K. (2004). A Tutorial on MM Algorithms. *The American Statistician*, **58**(1), 30–37.

1. Neal, R. M. (2011). [MCMC using Hamiltonian dynamics](https://www.mcmchandbook.net/HandbookChapter5.pdf). CRC press.

1. Polson, N. G., Scott, J. G. and Windle J. (2013). Bayesian inference for logistic models using Pólya–Gamma latent variables. *Journal of the American Statistical Association*, **108**(504), 1339--1349.

1. Roberts, G. O. and Rosenthal, J. S. (2001). Optimal scaling for various Metropolis-Hastings algorithms. *Statistical Science*, **16**(4), 351--367.

1. Roberts, G. O. and Rosenthal, J. S. (2009). Examples of adaptive MCMC. *Journal of Computational and Graphical Statistics*, **18**(2), 349--367.

------------------------------------------------

