{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasorigon/CompStat/blob/main/notebook/un_B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc5l6RlhtjOD"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96rchLtjtjOJ"
      },
      "outputs": [],
      "source": [
        "knitr::opts_chunk$set(echo = T, eval = T, message = F, warning = F, error = F, comment = NA, cache = F, include = T, R.options = list(width = 100), collapse = T, dpi = 200, fig.align = \"center\", fig.height = 6, fig.width = 8)\n",
        "\n",
        "Rcpp::sourceCpp(\"RMH.cpp\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vreYA8ITtjON"
      },
      "source": [
        "In this unit we discuss several advanced MCMC strategies that have been presented in the [**slides of unit B.2**](../slides/un_B2.pdf) and [**slides of unit C.2**](../slides/un_C2.pdf), including **MALA**, **Hamiltonian Monte Carlo** and the Gibbs sampling based on the **Pólya-gamma data augmentation**. \n",
        "\n",
        "We will implement these proposals using the \"famous\" **Pima indian dataset**, as in the previous [**Markdown document B.1**](un_B1.html). Let me stress again that the purpose of this unit is mainly presenting the implementation of the various MCMC algorithms and not making \"inference\" about their general performance. Refer to the nice paper by [Chopin & Ridgway (2017)](https://projecteuclid.org/journals/statistical-science/volume-32/issue-1/Leave-Pima-Indians-Alone--Binary-Regression-as-a-Benchmark/10.1214/16-STS581.full) for a more comprehensive discussion on this aspect. \n",
        "\n",
        "\n",
        "## Pima indian dataset\n",
        "\n",
        "We will make use of the Pima indian dataset again, as in the previous [**Markdown document B.1**](un_B1.html). Importantly, note that in this document we will **not standardize the predictors** to make the computational problem more challenging. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UICADQktjOO"
      },
      "outputs": [],
      "source": [
        "Pima <- rbind(MASS::Pima.tr, MASS::Pima.te)\n",
        "y <- as.numeric(Pima$type == \"Yes\") # Binary outcome\n",
        "X <- cbind(1, model.matrix(type ~ . - 1, data = Pima)) # Design matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUSanFmvtjOO"
      },
      "source": [
        "As done in [**Markdown document B.1**](un_B1.html), we will employ a relatively vague prior centered at $0$, namely $\\beta \\sim N(0, 100 I_p)$. Then, we implement the log-likelihood, the log-posterior and the **gradient** of the likelihood.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WID9I4TFtjOO"
      },
      "outputs": [],
      "source": [
        "# Log-likelihood of a logistic regression model\n",
        "loglik <- function(beta, y, X) {\n",
        "  eta <- c(X %*% beta)\n",
        "  sum(y * eta - log(1 + exp(eta)))\n",
        "}\n",
        "\n",
        "# Log-posterior\n",
        "logpost <- function(beta, y, X) {\n",
        "  loglik(beta, y, X) + sum(dnorm(beta, 0, 10, log = T))\n",
        "}\n",
        "\n",
        "# Gradient of the logposterior\n",
        "lgradient <- function(beta, y, X) {\n",
        "  probs <- plogis(c(X %*% beta))\n",
        "  loglik_gr <- c(crossprod(X, y - probs))\n",
        "  prior_gr <- -beta / 100\n",
        "  loglik_gr + prior_gr\n",
        "}\n",
        "\n",
        "# Summary table for the 6 considered methods\n",
        "summary_tab <- matrix(0, nrow = 6, ncol = 4)\n",
        "colnames(summary_tab) <- c(\"Seconds\", \"Average ESS\", \"Average ESS per sec\", \"Average acceptance rate\")\n",
        "rownames(summary_tab) <- c(\"MH Laplace + Rcpp\", \"MALA\", \"MALA tuned\", \"HMC\", \"STAN\", \"Pólya-Gamma\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XousrJDtjOP"
      },
      "source": [
        "## Metropolis Hastings (Laplace) and Rcpp\n",
        "\n",
        "We first consider a random walk Metropolis-Hastings algorithm based on *Rcpp* implementation. The source code can be found [**this file**](RMH.cpp). We again use a the Fisher information matrix as quick estimate for the covariance matrix. Refer to the [**slides of unit B.1**](../slides/un_B1.pdf) for more about this idea. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qUHtOCqtjOQ"
      },
      "outputs": [],
      "source": [
        "library(coda)\n",
        "R <- 30000\n",
        "burn_in <- 5000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vVZyPLGtjOQ"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZoviOuktjOR"
      },
      "outputs": [],
      "source": [
        "set.seed(123)\n",
        "\n",
        "# Covariance matrix is selected via laplace approximation\n",
        "fit_logit <- glm(y ~ X - 1, family = binomial(link = \"logit\"))\n",
        "p <- ncol(X)\n",
        "S <- 2.38^2 * vcov(fit_logit) / p\n",
        "\n",
        "# Running the MCMC\n",
        "start.time <- Sys.time()\n",
        "# MCMC\n",
        "fit_MCMC <- as.mcmc(RMH_arma(R, burn_in, y, X, S)) # Convert the matrix into a \"coda\" object\n",
        "end.time <- Sys.time()\n",
        "\n",
        "time_in_sec <- as.numeric(difftime(end.time, start.time, units = \"secs\"))\n",
        "\n",
        "# Diagnostic\n",
        "summary(effectiveSize(fit_MCMC)) # Effective sample size\n",
        "summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time\n",
        "summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate\n",
        "\n",
        "# Summary statistics\n",
        "summary_tab[1, ] <- c(\n",
        "  time_in_sec, mean(effectiveSize(fit_MCMC)),\n",
        "  mean(effectiveSize(fit_MCMC)) / time_in_sec,\n",
        "  1 - mean(rejectionRate(fit_MCMC))\n",
        ")\n",
        "\n",
        "# Traceplot of the intercept\n",
        "plot(fit_MCMC[, 1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTOd95rdtjOR"
      },
      "source": [
        "## MALA algoritm\n",
        "\n",
        "The MALA algorithm is described in the [**slides of unit B.2**](../slides/un_B2.pdf). Here we propose a general implementation which allows to specify a pre-conditioning matrix `S` and a scaling parameter `epsilon`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SABxbjH9tjOS"
      },
      "outputs": [],
      "source": [
        "# R represent the number of samples\n",
        "# burn_in is the number of discarded samples\n",
        "# epsilon, S are tuning parameter\n",
        "MALA <- function(R, burn_in, y, X, epsilon, S) {\n",
        "  p <- ncol(X)\n",
        "  out <- matrix(0, R, p) # Initialize an empty matrix to store the values\n",
        "  beta <- rep(0, p) # Initial values\n",
        "  A <- chol(S) # Cholesky of S\n",
        "  S1 <- solve(S) # Inverse of S\n",
        "\n",
        "  lgrad <- c(S %*% lgradient(beta, y, X)) # Compute the gradient\n",
        "  logp <- logpost(beta, y, X)\n",
        "\n",
        "  sigma2 <- epsilon^2 / p^(1 / 3)\n",
        "  sigma <- sqrt(sigma2)\n",
        "\n",
        "  # Starting the Gibbs sampling\n",
        "  for (r in 1:(burn_in + R)) {\n",
        "    beta_new <- beta + sigma2 / 2 * lgrad + sigma * c(crossprod(A, rnorm(p)))\n",
        "\n",
        "    logpnew <- logpost(beta_new, y, X)\n",
        "    lgrad_new <- c(S %*% lgradient(beta_new, y, X))\n",
        "\n",
        "    diffold <- beta - beta_new - sigma2 / 2 * lgrad_new\n",
        "    diffnew <- beta_new - beta - sigma2 / 2 * lgrad\n",
        "\n",
        "    qold <- -diffold %*% S1 %*% diffold / (2 * sigma2)\n",
        "    qnew <- -diffnew %*% S1 %*% diffnew / (2 * sigma2)\n",
        "\n",
        "    alpha <- min(1, exp(logpnew - logp + qold - qnew))\n",
        "    if (runif(1) < alpha) {\n",
        "      logp <- logpnew\n",
        "      lgrad <- lgrad_new\n",
        "      beta <- beta_new # Accept the value\n",
        "    }\n",
        "    # Store the values after the burn-in period\n",
        "    if (r > burn_in) {\n",
        "      out[r - burn_in, ] <- beta\n",
        "    }\n",
        "  }\n",
        "  out\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqXjGN5CtjOS"
      },
      "source": [
        "The first run of the MALA algorithm is performed using a diagonal matrix (i.e. no pre-conditioning). As detailed in the [**slides of unit B.2**](../slides/un_B2.pdf), this expected to perform poorly. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw6WosuQtjOT"
      },
      "outputs": [],
      "source": [
        "set.seed(123)\n",
        "\n",
        "epsilon <- 0.0017 # After some trial ad error\n",
        "\n",
        "# Running the MCMC\n",
        "start.time <- Sys.time()\n",
        "fit_MCMC <- as.mcmc(MALA(R = R, burn_in = burn_in, y, X, epsilon, S = diag(ncol(X)))) # Convert the matrix into a \"coda\" object\n",
        "end.time <- Sys.time()\n",
        "time_in_sec <- as.numeric(difftime(end.time, start.time, units = \"secs\"))\n",
        "\n",
        "# Diagnostic\n",
        "summary(effectiveSize(fit_MCMC)) # Effective sample size\n",
        "summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time\n",
        "summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate\n",
        "\n",
        "# Summary statistics\n",
        "summary_tab[2, ] <- c(\n",
        "  time_in_sec, mean(effectiveSize(fit_MCMC)),\n",
        "  mean(effectiveSize(fit_MCMC)) / time_in_sec,\n",
        "  1 - mean(rejectionRate(fit_MCMC))\n",
        ")\n",
        "\n",
        "# Traceplot of the intercept\n",
        "plot(fit_MCMC[, 1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn3BsMyXtjOT"
      },
      "source": [
        "## MALA algorithm with pre-conditioning\n",
        "\n",
        "In this second implementation of MALA, we rely instead on the Fisher information matrix, as in the RWM example. This indeed leads to much better results. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81-B9I56tjOU"
      },
      "outputs": [],
      "source": [
        "library(coda)\n",
        "R <- 30000\n",
        "burn_in <- 5000\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF-YL4bStjOU"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULjWpBwwtjOV"
      },
      "outputs": [],
      "source": [
        "set.seed(123)\n",
        "\n",
        "epsilon <- 1.68 # After some trial ad error\n",
        "\n",
        "# Covariance matrix is selected via laplace approximation\n",
        "fit_logit <- glm(y ~ X - 1, family = binomial(link = \"logit\"))\n",
        "S <- vcov(fit_logit)\n",
        "\n",
        "# Running the MCMC\n",
        "start.time <- Sys.time()\n",
        "fit_MCMC <- as.mcmc(MALA(R = R, burn_in = burn_in, y, X, epsilon, S)) # Convert the matrix into a \"coda\" object\n",
        "end.time <- Sys.time()\n",
        "time_in_sec <- as.numeric(difftime(end.time, start.time, units = \"secs\"))\n",
        "\n",
        "# Diagnostic\n",
        "summary(effectiveSize(fit_MCMC)) # Effective sample size\n",
        "summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time\n",
        "summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate\n",
        "\n",
        "# Summary statistics\n",
        "summary_tab[3, ] <- c(\n",
        "  time_in_sec, mean(effectiveSize(fit_MCMC)),\n",
        "  mean(effectiveSize(fit_MCMC)) / time_in_sec,\n",
        "  1 - mean(rejectionRate(fit_MCMC))\n",
        ")\n",
        "\n",
        "# Traceplot of the intercept\n",
        "plot(fit_MCMC[, 1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upyokGHTtjOV"
      },
      "source": [
        "## Hamiltonian Monte Carlo\n",
        "\n",
        "The following `HMC` function is an implementation of Hamiltonian Monte Carlo for a general parametric model. As before, the object `S` represents a pre-conditioning matrix. This code is an adaptation of the one written by [**Neal (2011)**](https://www.mcmchandbook.net/HandbookChapter5.pdf). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgEfsTf9tjOW"
      },
      "outputs": [],
      "source": [
        "HMC <- function(R, burn_in, y, X, epsilon, S, L = 10) {\n",
        "  p <- ncol(X)\n",
        "  out <- matrix(0, R, p) # Initialize an empty matrix to store the values\n",
        "  beta <- rep(0, p) # Initial values\n",
        "  logp <- logpost(beta, y, X) # Initial log-posterior\n",
        "  S1 <- solve(S)\n",
        "  A1 <- chol(S1)\n",
        "\n",
        "  # Starting the Gibbs sampling\n",
        "  for (r in 1:(burn_in + R)) {\n",
        "    P <- c(crossprod(A1, rnorm(p))) # Auxiliary variables\n",
        "    logK <- c(P %*% S %*% P / 2) # Kinetic energy at the beginning of the trajectory\n",
        "\n",
        "    # Make a half step for momentum at the beginning\n",
        "    beta_new <- beta\n",
        "    Pnew <- P + epsilon * lgradient(beta_new, y, X) / 2\n",
        "\n",
        "    # Alternate full steps for position and momentum\n",
        "    for (l in 1:L) {\n",
        "      # Make a full step for the position\n",
        "      beta_new <- beta_new + epsilon * c(S %*% Pnew)\n",
        "      # Make a full step for the momentum, except at end of trajectory\n",
        "      if (l != L) Pnew <- Pnew + epsilon * lgradient(beta_new, y, X)\n",
        "    }\n",
        "    # Make a half step for momentum at the end.\n",
        "    Pnew <- Pnew + epsilon * lgradient(beta_new, y, X) / 2\n",
        "\n",
        "    # Negate momentum at end of trajectory to make the proposal symmetric\n",
        "    Pnew <- - Pnew\n",
        "\n",
        "    # Evaluate potential and kinetic energies at the end of trajectory\n",
        "    logpnew <- logpost(beta_new, y, X)\n",
        "    logKnew <- Pnew %*% S %*% Pnew / 2 \n",
        "\n",
        "    # Accept or reject the state at end of trajectory, returning either\n",
        "    # the position at the end of the trajectory or the initial position\n",
        "    if (runif(1) < exp(logpnew - logp + logK - logKnew)) {\n",
        "      logp <- logpnew\n",
        "      beta <- beta_new # Accept the value\n",
        "    }\n",
        "\n",
        "    # Store the values after the burn-in period\n",
        "    if (r > burn_in) {\n",
        "      out[r - burn_in, ] <- beta\n",
        "    }\n",
        "  }\n",
        "  out\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40krQ9IRtjOW"
      },
      "source": [
        "We run the `HMC` using the usual Fisher information matrix as covariance matrix. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX17gVt1tjOX"
      },
      "outputs": [],
      "source": [
        "set.seed(123)\n",
        "\n",
        "epsilon <- 0.25 # After some trial ad error\n",
        "L <- 10\n",
        "\n",
        "# Covariance matrix is selected via laplace approximation\n",
        "fit_logit <- glm(y ~ X - 1, family = binomial(link = \"logit\"))\n",
        "S <- vcov(fit_logit)\n",
        "\n",
        "# Running the MCMC\n",
        "start.time <- Sys.time()\n",
        "fit_MCMC <- as.mcmc(HMC(R = R, burn_in = burn_in, y, X, epsilon, S, L)) # Convert the matrix into a \"coda\" object\n",
        "end.time <- Sys.time()\n",
        "time_in_sec <- as.numeric(difftime(end.time, start.time, units = \"secs\"))\n",
        "\n",
        "# Diagnostic\n",
        "summary(effectiveSize(fit_MCMC)) # Effective sample size\n",
        "summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time\n",
        "summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate\n",
        "\n",
        "# Summary statistics\n",
        "summary_tab[4, ] <- c(\n",
        "  time_in_sec, mean(effectiveSize(fit_MCMC)),\n",
        "  mean(effectiveSize(fit_MCMC)) / time_in_sec,\n",
        "  1 - mean(rejectionRate(fit_MCMC))\n",
        ")\n",
        "\n",
        "# Traceplot of the intercept\n",
        "plot(fit_MCMC[, 1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pK1RnE5tjOX"
      },
      "source": [
        "## Hamiltonian Monte Carlo (Stan)\n",
        "\n",
        "For the sake of completeness, we also present the results obtained using the **Stan** software, which must be installed alongside the `rstan` R package. The [`logistic.stan`](logistic.stan) is available in this repository. The following chunk of code compile the model in C++. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEnerTh4tjOX"
      },
      "outputs": [],
      "source": [
        "library(rstan)\n",
        "\n",
        "# I am not counting the compilation time\n",
        "stan_compiled <- stan_model(file = \"logistic.stan\") # Stan program\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5fHiJ58tjOY"
      },
      "source": [
        "We then sample the MCMC values and store the results.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgLu0fwTtjOY"
      },
      "outputs": [],
      "source": [
        "set.seed(1234)\n",
        "# Running the MCMC\n",
        "start.time <- Sys.time()\n",
        "fit_HMC <- sampling(\n",
        "  stan_compiled, # The stan file has been previously compiled\n",
        "  data = list(X = X, y = y, n = nrow(X), p = ncol(X)), # named list of data\n",
        "  chains = 1, # number of Markov chains\n",
        "  warmup = burn_in, # Burn-in iterations per chain\n",
        "  iter = R + burn_in # Total number of iterations per chain\n",
        ")\n",
        "end.time <- Sys.time()\n",
        "time_in_sec <- as.numeric(difftime(end.time, start.time, units = \"secs\"))\n",
        "\n",
        "fit_HMC <- as.mcmc(extract(fit_HMC)$beta)\n",
        "\n",
        "# Diagnostic\n",
        "summary(effectiveSize(fit_HMC)) # Effective sample size\n",
        "summary(R / effectiveSize(fit_HMC)) # Integrated autocorrelation time\n",
        "summary(1 - rejectionRate(fit_HMC)) # Acceptance rate\n",
        "\n",
        "# Summary statistics\n",
        "summary_tab[5, ] <- c(\n",
        "  time_in_sec, mean(effectiveSize(fit_HMC)),\n",
        "  mean(effectiveSize(fit_HMC)) / time_in_sec,\n",
        "  1 - mean(rejectionRate(fit_HMC))\n",
        ")\n",
        "\n",
        "# Traceplot of the intercept\n",
        "plot(fit_HMC[, 1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6g7DMsetjOY"
      },
      "source": [
        "## Pólya-gamma data-augmentation\n",
        "\n",
        "The Pólya-gamma data-augmentation is described in the paper [Polson, N. G., Scott, J. G. and Windle J. (2013)](https://www.tandfonline.com/doi/full/10.1080/01621459.2013.829001). The simulation of the Pólya-gamma random variables is handled by the function `rpg.devroye` within the `BayesLogit` R package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwvT-kDktjOZ"
      },
      "outputs": [],
      "source": [
        "library(BayesLogit)\n",
        "\n",
        "logit_Gibbs <- function(R, burn_in, y, X, B, b) {\n",
        "  p <- ncol(X)\n",
        "  n <- nrow(X)\n",
        "  out <- matrix(0, R, p) # Initialize an empty matrix to store the values\n",
        "\n",
        "  P <- solve(B) # Prior precision matrix\n",
        "  Pb <- P %*% b # Term appearing in the Gibbs sampling\n",
        "\n",
        "  Xy <- crossprod(X, y - 1 / 2)\n",
        "\n",
        "  # Initialization\n",
        "  beta <- rep(0, p)\n",
        "\n",
        "  # Iterative procedure\n",
        "  for (r in 1:(R + burn_in)) {\n",
        "\n",
        "    # Sampling the Pólya-gamma latent variables\n",
        "    eta <- c(X %*% beta)\n",
        "    omega <- rpg.devroye(num = n, h = 1, z = eta)\n",
        "\n",
        "    # Sampling beta\n",
        "    eig <- eigen(crossprod(X * sqrt(omega)) + P, symmetric = TRUE)\n",
        "\n",
        "    Sigma <- crossprod(t(eig$vectors) / sqrt(eig$values))\n",
        "    mu <- Sigma %*% (Xy + Pb)\n",
        "\n",
        "    A1 <- t(eig$vectors) / sqrt(eig$values)\n",
        "    beta <- mu + c(matrix(rnorm(1 * p), 1, p) %*% A1)\n",
        "\n",
        "    # Store the values after the burn-in period\n",
        "    if (r > burn_in) {\n",
        "      out[r - burn_in, ] <- beta\n",
        "    }\n",
        "  }\n",
        "  out\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSR2MM7OtjOZ"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtaWutVmtjOa"
      },
      "outputs": [],
      "source": [
        "B <- diag(100, ncol(X)) # Prior covariance matrix\n",
        "b <- rep(0, ncol(X)) # Prior mean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMqR-ZOQtjOa"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Puqp6K_UtjOa"
      },
      "outputs": [],
      "source": [
        "set.seed(123)\n",
        "\n",
        "# Running the MCMC\n",
        "start.time <- Sys.time()\n",
        "fit_MCMC <- as.mcmc(logit_Gibbs(R, burn_in, y, X, B, b)) # Convert the matrix into a \"coda\" object\n",
        "end.time <- Sys.time()\n",
        "time_in_sec <- as.numeric(difftime(end.time, start.time, units = \"secs\"))\n",
        "\n",
        "# Diagnostic\n",
        "summary(effectiveSize(fit_MCMC)) # Effective sample size\n",
        "summary(R / effectiveSize(fit_MCMC)) # Integrated autocorrelation time\n",
        "summary(1 - rejectionRate(fit_MCMC)) # Acceptance rate\n",
        "\n",
        "# Summary statistics\n",
        "summary_tab[6, ] <- c(\n",
        "  time_in_sec, mean(effectiveSize(fit_MCMC)),\n",
        "  mean(effectiveSize(fit_MCMC)) / time_in_sec,\n",
        "  1 - mean(rejectionRate(fit_MCMC))\n",
        ")\n",
        "\n",
        "\n",
        "# Traceplot of the intercept\n",
        "plot(fit_MCMC[, 1:2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hqFocsFtjOb"
      },
      "source": [
        "## Results\n",
        "\n",
        "The summary statistics of all the above algorithm are reported in the table below. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO8A9nkXtjOb"
      },
      "outputs": [],
      "source": [
        "knitr::kable(summary_tab)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "langauge": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "colab": {
      "name": "un_B2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}