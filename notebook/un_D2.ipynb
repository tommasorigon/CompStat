{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommasorigon/CompStat/blob/main/notebook/un_D2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRzIryWPry6J"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnZIDCcjry6P"
      },
      "outputs": [],
      "source": [
        "knitr::opts_chunk$set(echo = T, eval = T, message = F, warning = F, error = F, comment = NA, cache = F, include = T, R.options = list(width = 100), collapse = T, dpi = 200, fig.align = \"center\", fig.height = 6, fig.width = 8)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9oJihUjry6S"
      },
      "source": [
        "In this unit we discuss several approximate Bayesian methods that have been presented in the [**slides of unit D.2**](../slides/un_D2.pdf). We will make use of the Pima indian dataset again, as in the previous [**Markdown document B.1**](un_B1.html) and [**Markdown document B.2**](un_B2.html). Importantly, note that in this document we will **not standardize the predictors** to make the computational problem more challenging. \n",
        "\n",
        "\n",
        "## The Pima indian dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfLWzCEsry6T"
      },
      "outputs": [],
      "source": [
        "Pima <- rbind(MASS::Pima.tr, MASS::Pima.te)\n",
        "y <- as.numeric(Pima$type == \"Yes\") # Binary outcome\n",
        "X <- cbind(1, model.matrix(type ~ . - 1, data = Pima)) # Design matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6fS53EXry6T"
      },
      "source": [
        "We will employ a relatively vague prior centered at $0$, namely $\\beta \\sim N(0, 100 I_p)$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMpGdXqXry6U"
      },
      "outputs": [],
      "source": [
        "B <- diag(10, ncol(X)) # Prior covariance matrix\n",
        "b <- rep(0, ncol(X)) # Prior mean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToqIvS8jry6U"
      },
      "source": [
        "## The gold standard: MCMC\n",
        "\n",
        "In order to assess the accuracy of the approximations, we first obtain a large number of posterior samples using the Pólya-gamma data augmentation approach. This is the same code that is reported also in the last paragraph of the [**Markdown document B.2**](un_B2.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_wSKR-Dry6V"
      },
      "outputs": [],
      "source": [
        "library(BayesLogit)\n",
        "logit_Gibbs <- function(R, burn_in, y, X, B, b) {\n",
        "  p <- ncol(X)\n",
        "  n <- nrow(X)\n",
        "  out <- matrix(0, R, p) # Initialize an empty matrix to store the values\n",
        "\n",
        "  P <- solve(B) # Prior precision matrix\n",
        "  Pb <- P %*% b # Term appearing in the Gibbs sampling\n",
        "\n",
        "  Xy <- crossprod(X, y - 1 / 2)\n",
        "\n",
        "  # Initialization\n",
        "  beta <- rep(0, p)\n",
        "\n",
        "  # Iterative procedure\n",
        "  for (r in 1:(R + burn_in)) {\n",
        "\n",
        "    # Sampling the Pólya-gamma latent variables\n",
        "    eta <- c(X %*% beta)\n",
        "    omega <- rpg.devroye(num = n, h = 1, z = eta)\n",
        "\n",
        "    # Sampling beta\n",
        "    eig <- eigen(crossprod(X * sqrt(omega)) + P, symmetric = TRUE)\n",
        "\n",
        "    Sigma <- crossprod(t(eig$vectors) / sqrt(eig$values))\n",
        "    mu <- Sigma %*% (Xy + Pb)\n",
        "\n",
        "    A1 <- t(eig$vectors) / sqrt(eig$values)\n",
        "    beta <- mu + c(matrix(rnorm(1 * p), 1, p) %*% A1)\n",
        "\n",
        "    # Store the values after the burn-in period\n",
        "    if (r > burn_in) {\n",
        "      out[r - burn_in, ] <- beta\n",
        "    }\n",
        "  }\n",
        "  out\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2AQzjj2ry6V"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njjnmz9_ry6W"
      },
      "outputs": [],
      "source": [
        "set.seed(123)\n",
        "\n",
        "# Running the MCMC\n",
        "fit_MCMC <- logit_Gibbs(R = 10^5, burn_in = 5000, y, X, B, b) # MCMC (gold standard)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4CYYZPrry6W"
      },
      "source": [
        "## Laplace approximation\n",
        "\n",
        "The first approach considered is the Laplace approximation. Note that the MAP is obtained using the **Pólya-gamma data-augmentation**, therefore ensuring a monotonic procedure. Refer to the [**slides of unit D.2**](../slides/un_D2.pdf) for further details. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XONrmbLxry6X"
      },
      "outputs": [],
      "source": [
        "logit_Laplace <- function(y, X, B, b, tol = 1e-16, maxiter = 10000) {\n",
        "  P <- solve(B) # Prior precision matrix\n",
        "  Pb <- P %*% b # Term appearing in the Gibbs sampling\n",
        "\n",
        "  logpost <- numeric(maxiter)\n",
        "  Xy <- crossprod(X, y - 0.5)\n",
        "\n",
        "  # Initialization\n",
        "  beta <- solve(crossprod(X / 4, X) + P, Xy + Pb)\n",
        "  eta <- c(X %*% beta)\n",
        "  w <- tanh(eta / 2) / (2 * eta)\n",
        "  w[is.nan(w)] <- 0.25\n",
        "\n",
        "  # First value of the likelihood\n",
        "  logpost[1] <- sum(y * eta - log(1 + exp(eta))) - 0.5 * t(beta) %*% P %*% beta\n",
        "\n",
        "  # Iterative procedure\n",
        "  for (t in 2:maxiter) {\n",
        "    beta <- solve(qr(crossprod(X * w, X) + P), Xy + Pb)\n",
        "    eta <- c(X %*% beta)\n",
        "    w <- tanh(eta / 2) / (2 * eta)\n",
        "    w[is.nan(w)] <- 0.25\n",
        "\n",
        "    logpost[t] <- sum(y * eta - log(1 + exp(eta))) - 0.5 * t(beta) %*% P %*% beta\n",
        "\n",
        "    if (logpost[t] - logpost[t - 1] < tol) {\n",
        "      prob <- plogis(eta)\n",
        "      return(list(\n",
        "        mu = c(beta), Sigma = solve(crossprod(X * prob * (1 - prob), X) + P),\n",
        "        Convergence = cbind(Iteration = (1:t) - 1, logpost = logpost[1:t])\n",
        "      ))\n",
        "    }\n",
        "  }\n",
        "  stop(\"The algorithm has not reached convergence\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPxLt2eAry6X"
      },
      "source": [
        "The marginal densities of the first two regression coefficients are reported, together with the \"gold standard\" obtained from the MCMC samples. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9GK9isxry6Y"
      },
      "outputs": [],
      "source": [
        "library(tictoc)\n",
        "\n",
        "tic()\n",
        "fit_Laplace <- logit_Laplace(y, X, B, b)\n",
        "toc()\n",
        "\n",
        "par(mfrow = c(1, 2))\n",
        "plot(density(fit_MCMC[, 1]), xlab = expression(beta[1]), lty = \"dotted\", main = \"\")\n",
        "curve(dnorm(x, fit_Laplace$mu[1], sqrt(fit_Laplace$Sigma[1, 1])), add = T)\n",
        "\n",
        "plot(density(fit_MCMC[, 2]), xlab = expression(beta[2]), lty = \"dotted\", main = \"\")\n",
        "curve(dnorm(x, fit_Laplace$mu[2], sqrt(fit_Laplace$Sigma[2, 2])), add = T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKNEArkFry6Y"
      },
      "source": [
        "## Variational Bayes\n",
        "\n",
        "The second approximation is the Variational Bayes approximation of [**Jaakkola and Jordan (2000)**](https://link.springer.com/article/10.1023/A:1008932416310) and later considered also by [**Durante and Rigon (2019)**](https://projecteuclid.org/euclid.ss/1570780980). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C30JsQ5ery6Z"
      },
      "outputs": [],
      "source": [
        "# Compute the log-determinant of a matrix\n",
        "ldet <- function(X) {\n",
        "  if (!is.matrix(X)) {\n",
        "    return(log(X))\n",
        "  }\n",
        "  determinant(X, logarithm = TRUE)$modulus\n",
        "}\n",
        "\n",
        "logit_CAVI <- function(y, X, B, b, tol = 1e-16, maxiter = 10000) {\n",
        "  lowerbound <- numeric(maxiter)\n",
        "  p <- ncol(X)\n",
        "  n <- nrow(X)\n",
        "\n",
        "  P <- solve(B)\n",
        "  Pb <- c(P %*% b)\n",
        "  Pdet <- ldet(P)\n",
        "\n",
        "  # Initialization for omega equal to 0.25\n",
        "  P_vb <- crossprod(X * rep(1 / 4, n), X) + P\n",
        "  Sigma_vb <- solve(P_vb)\n",
        "  mu_vb <- Sigma_vb %*% (crossprod(X, y - 0.5) + Pb)\n",
        "  eta <- c(X %*% mu_vb)\n",
        "  xi <- sqrt(eta^2 + rowSums(X %*% Sigma_vb * X))\n",
        "  omega <- tanh(xi / 2) / (2 * xi)\n",
        "  omega[is.nan(omega)] <- 0.25\n",
        "\n",
        "  lowerbound[1] <- 0.5 * p + 0.5 * ldet(Sigma_vb) + 0.5 * Pdet - 0.5 * t(mu_vb - b) %*% P %*% (mu_vb - b) + sum((y - 0.5) * eta + log(plogis(xi)) - 0.5 * xi) - 0.5 * sum(diag(P %*% Sigma_vb))\n",
        "\n",
        "  # Iterative procedure\n",
        "  for (t in 2:maxiter) {\n",
        "    P_vb <- crossprod(X * omega, X) + P\n",
        "    Sigma_vb <- solve(P_vb)\n",
        "    mu_vb <- Sigma_vb %*% (crossprod(X, y - 0.5) + Pb)\n",
        "\n",
        "    # Update of xi\n",
        "    eta <- c(X %*% mu_vb)\n",
        "    xi <- sqrt(eta^2 + rowSums(X %*% Sigma_vb * X))\n",
        "    omega <- tanh(xi / 2) / (2 * xi)\n",
        "    omega[is.nan(omega)] <- 0.25\n",
        "\n",
        "    lowerbound[t] <- 0.5 * p + 0.5 * ldet(Sigma_vb) + 0.5 * Pdet - 0.5 * t(mu_vb - b) %*% P %*% (mu_vb - b) + sum((y - 0.5) * eta + log(plogis(xi)) - 0.5 * xi) - 0.5 * sum(diag(P %*% Sigma_vb))\n",
        "\n",
        "    if (abs(lowerbound[t] - lowerbound[t - 1]) < tol) {\n",
        "      return(list(\n",
        "        mu = c(mu_vb), Sigma = matrix(Sigma_vb, p, p),\n",
        "        Convergence = cbind(\n",
        "          Iteration = (1:t) - 1,\n",
        "          Lowerbound = lowerbound[1:t]\n",
        "        ), xi = xi\n",
        "      ))\n",
        "    }\n",
        "  }\n",
        "  stop(\"The algorithm has not reached convergence\")\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwaFdYgRry6a"
      },
      "source": [
        "The marginal densities of the first two regression coefficients are reported, together with the \"gold standard\" obtained from the MCMC samples. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W11I2V4Lry6a"
      },
      "outputs": [],
      "source": [
        "tic()\n",
        "fit_CAVI <- logit_CAVI(y, X, B, b)\n",
        "toc()\n",
        "\n",
        "par(mfrow = c(1, 2))\n",
        "plot(density(fit_MCMC[, 1]), xlab = expression(beta[1]), lty = \"dotted\", main = \"\", ylim = c(0, 0.6))\n",
        "curve(dnorm(x, fit_CAVI$mu[1], sqrt(fit_CAVI$Sigma[1, 1])), add = T)\n",
        "\n",
        "plot(density(fit_MCMC[, 2]), xlab = expression(beta[2]), lty = \"dotted\", main = \"\", ylim = c(0, 12))\n",
        "curve(dnorm(x, fit_CAVI$mu[2], sqrt(fit_CAVI$Sigma[2, 2])), add = T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miLSmgulry6b"
      },
      "source": [
        "## Expectation propagation\n",
        "\n",
        "The third approximation is the expectation propagation, which is described in the logistic regression case for example by [**Chopin and Ridgway (2017)**](https://projecteuclid.org/journals/statistical-science/volume-32/issue-1/Leave-Pima-Indians-Alone--Binary-Regression-as-a-Benchmark/10.1214/16-STS581.full).\n",
        "\n",
        "Note: the `EPGLM` package that compute the EP approximation using the `EPlogit` command  is not anymore available on the CRAN. However, the source code can be [**downloaded here**](https://cran.r-project.org/src/contrib/Archive/EPGLM/EPGLM_1.1.2.tar.gz) and installed manually.  \n",
        "\n",
        "The marginal densities of the first two regression coefficients are reported, together with the \"gold standard\" obtained from the MCMC samples. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPv_2j4Xry6b"
      },
      "outputs": [],
      "source": [
        "library(EPGLM)\n",
        "\n",
        "tic()\n",
        "fit_EP <- EPlogit(X, y, s = B[1, 1])\n",
        "toc()\n",
        "\n",
        "par(mfrow = c(1, 2))\n",
        "plot(density(fit_MCMC[, 1]), xlab = expression(beta[1]), lty = \"dotted\", main = \"\")\n",
        "curve(dnorm(x, fit_EP$m[1], sqrt(fit_EP$V[1, 1])), add = T)\n",
        "\n",
        "plot(density(fit_MCMC[, 2]), xlab = expression(beta[2]), lty = \"dotted\", main = \"\")\n",
        "curve(dnorm(x, fit_EP$m[2], sqrt(fit_EP$V[2, 2])), add = T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP6imTjBry6c"
      },
      "source": [
        "## Hybrid Laplace\n",
        "\n",
        "THe fourth approach is based on the variational Bayes approximation mean estimate which is plugged-in into the Fisher information matrix. This procedure is described in the [**slides of unit D.2**](../slides/un_D2.pdf). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeYiLH-qry6c"
      },
      "outputs": [],
      "source": [
        "logit_HL <- function(y, X, B, b, tol = 1e-16, maxiter = 10000) {\n",
        "  fit_HL <- logit_CAVI(y, X, B, b, tol, maxiter)\n",
        "  prob <- c(plogis(X %*% fit_CAVI$mu))\n",
        "  fit_HL$Sigma <- solve(crossprod(X * prob * (1 - prob), X) + solve(B))\n",
        "  fit_HL\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X2MobTzry6d"
      },
      "source": [
        "The marginal densities of the first two regression coefficients are reported, together with the \"gold standard\" obtained from the MCMC samples. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQL1taj8ry6d"
      },
      "outputs": [],
      "source": [
        "fit_HL <- logit_HL(y, X, B, b)\n",
        "\n",
        "par(mfrow = c(1, 2))\n",
        "plot(density(fit_MCMC[, 1]), xlab = expression(beta[1]), lty = \"dotted\", main = \"\")\n",
        "curve(dnorm(x, fit_HL$mu[1], sqrt(fit_HL$Sigma[1, 1])), add = T)\n",
        "\n",
        "plot(density(fit_MCMC[, 2]), xlab = expression(beta[2]), lty = \"dotted\", main = \"\")\n",
        "curve(dnorm(x, fit_HL$mu[2], sqrt(fit_HL$Sigma[2, 2])), add = T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On5VDcuWry6d"
      },
      "source": [
        "## Further comparisons\n",
        "\n",
        "In the following, we report the posterior means and variances under the all the previous approximations together with the gold standard MCMC values. \n",
        "\n",
        "### Mean and variances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByXXbtTnry6e"
      },
      "outputs": [],
      "source": [
        "mu_MCMC <- colMeans(fit_MCMC)\n",
        "Sigma_MCMC <- var(fit_MCMC)\n",
        "\n",
        "Means <- data.frame(\n",
        "  MCMC = mu_MCMC,\n",
        "  Laplace = fit_Laplace$mu,\n",
        "  VB = fit_CAVI$mu,\n",
        "  EP = fit_EP$m,\n",
        "  HL = fit_HL$mu\n",
        ")\n",
        "knitr::kable(Means, digits = 4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fxjwRGMry6e"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXWFR3eBry6e"
      },
      "outputs": [],
      "source": [
        "Sd <- data.frame(\n",
        "  MCMC = sqrt(diag(var(fit_MCMC))),\n",
        "  Laplace = sqrt(diag(fit_Laplace$Sigma)),\n",
        "  VB = sqrt(diag(fit_CAVI$Sigma)),\n",
        "  EP = sqrt(diag(fit_EP$V)),\n",
        "  HL = sqrt(diag(fit_HL$Sigma))\n",
        ")\n",
        "knitr::kable(Sd, digits = 4, row.names = F)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rS7vLBfry6f"
      },
      "source": [
        "### Discrepancy from the optimal Gaussian\n",
        "\n",
        "In the last table, we comput the Kullback-Leibler divergence between the obtained Gaussian approximations and the \"optimal\" Gaussian distribution, which is the one matching the correct posterior mean and variance. These quantities are approximated using MCMC. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKSp8CSSry6f"
      },
      "outputs": [],
      "source": [
        "KL_gauss <- function(mu1, Sigma1, mu2, Sigma2) {\n",
        "  p <- ncol(Sigma1)\n",
        "  c(0.5 * (ldet(Sigma2) - ldet(Sigma1) - p + sum(diag(solve(Sigma2) %*% Sigma1)) + t(mu2 - mu1) %*% solve(Sigma2) %*% (mu2 - mu1)))\n",
        "}\n",
        "\n",
        "library(expm)\n",
        "dWass_gauss <- function(mu1, Sigma1, mu2, Sigma2) {\n",
        "  Sigma2_r <- sqrtm(Sigma2)\n",
        "  c(crossprod(mu2 - mu1) + sum(diag(Sigma1 + Sigma2 - 2 * sqrtm(Sigma2_r %*% Sigma1 %*% Sigma2_r))))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NedBSQwQry6f"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVaCxH2rry6g"
      },
      "outputs": [],
      "source": [
        "tab <- data.frame(\n",
        "  Type = c(\"Laplace\", \"Variational Bayes\", \"Expectation Propagation\", \"Hybrid Laplace\"),\n",
        "  KL = c(\n",
        "    KL_gauss(fit_Laplace$mu, fit_Laplace$Sigma, mu_MCMC, Sigma_MCMC),\n",
        "    KL_gauss(fit_CAVI$mu, fit_CAVI$Sigma, mu_MCMC, Sigma_MCMC),\n",
        "    KL_gauss(fit_EP$m, fit_EP$V, mu_MCMC, Sigma_MCMC),\n",
        "    KL_gauss(fit_HL$mu, fit_HL$Sigma, mu_MCMC, Sigma_MCMC)\n",
        "  ),\n",
        "  Wasserstein = c(\n",
        "    dWass_gauss(mu_MCMC, Sigma_MCMC, fit_Laplace$mu, fit_Laplace$Sigma),\n",
        "    dWass_gauss(mu_MCMC, Sigma_MCMC, fit_CAVI$mu, fit_CAVI$Sigma),\n",
        "    dWass_gauss(mu_MCMC, Sigma_MCMC, fit_EP$m, fit_EP$V),\n",
        "    dWass_gauss(mu_MCMC, Sigma_MCMC, fit_HL$mu, fit_HL$Sigma)\n",
        "  )\n",
        ")\n",
        "knitr::kable(tab, digits = 4)\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": "",
    "kernelspec": {
      "display_name": "R",
      "langauge": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "colab": {
      "name": "un_D2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}